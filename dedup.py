import os
import sqlite3
from config import Config
from utils import get_file_hash, encrypt_file, log_action, upload_to_s3

class Deduplicator:
    def __init__(self):
        self.db_path = Config.DATABASE
        self.stored_dir = Config.UPLOAD_STORED

    def process_file(self, temp_path, file_name, user_id):
        """
        Process an uploaded file: Hash -> Check Dedup -> Encrypt -> Store
        Returns (is_duplicate, file_id)
        """
        file_hash = get_file_hash(temp_path)
        file_size = os.path.getsize(temp_path)
        file_type = file_name.split('.')[-1] if '.' in file_name else 'unknown'

        # Check for deduplication
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT id, stored_path FROM files WHERE file_hash = ?", (file_hash,))
        existing_file = cursor.fetchone()

        if existing_file:
            # Duplicate found
            file_id = existing_file[0]
            log_action("Deduplication", f"Duplicate detected for {file_name} (Hash: {file_hash}). Referencing existing file ID: {file_id}")
            
            # Record the upload for this user
            cursor.execute("INSERT INTO uploads (user_id, file_id) VALUES (?, ?)", (user_id, file_id))
            conn.commit()
            conn.close()
            log_action("Deduplication Success", f"Duplicate detected for {file_name} (Hash: {file_hash}). Referencing existing file ID: {file_id}")
            return True, file_id
        else:
            # Unique file
            # Limit filename length to prevent Windows path issues (max 260 chars)
            # Keep hash (64 chars) + extension, limit original filename
            file_extension = os.path.splitext(file_name)[1]  # e.g., '.pdf'
            base_name = os.path.splitext(file_name)[0]  # filename without extension
            
            # Limit base name to 50 characters to keep total path under Windows limit
            max_base_length = 50
            if len(base_name) > max_base_length:
                base_name = base_name[:max_base_length]
            
            stored_file_name = f"{file_hash}_{base_name}{file_extension}"
            stored_path = os.path.join(self.stored_dir, stored_file_name)
            
            # Ensure stored directory exists
            if not os.path.exists(self.stored_dir):
                os.makedirs(self.stored_dir, exist_ok=True)
            
            # Encrypt and move to stored_files (local temp before S3)
            encrypt_file(temp_path, stored_path)
            
            # S3 Integration
            final_path = stored_path
            if Config.USE_S3:
                s3_object_name = stored_file_name
                if upload_to_s3(stored_path, s3_object_name):
                    log_action("Cloud Sync", f"File {file_name} synced to S3 bucket.")
                    final_path = f"s3://{Config.S3_BUCKET_NAME}/{s3_object_name}"
                else:
                    log_action("Cloud Error", f"Failed to sync {file_name} to S3. Keeping local as backup.")
                
                # Direct S3 requirement: Always remove local encrypted copy if S3 upload was attempted
                # even if it failed, we don't want permanent local storage if the user asked for "Direct S3"
                # However, for safety, we only remove if it's already in S3 or if we want to honor "Direct S3" strictly.
                if os.path.exists(stored_path):
                    os.remove(stored_path) 
            
            # Update database
            cursor.execute("""
                INSERT INTO files (file_name, file_hash, file_size, file_type, stored_path)
                VALUES (?, ?, ?, ?, ?)
            """, (file_name, file_hash, file_size, file_type, final_path))
            
            file_id = cursor.lastrowid
            
            cursor.execute("INSERT INTO uploads (user_id, file_id) VALUES (?, ?)", (user_id, file_id))
            
            conn.commit()
            conn.close()
            
            log_action("Upload", f"New file stored: {file_name} (ID: {file_id})")
            return False, file_id

    def proof_of_ownership(self, user_id, file_hash):
        """Simulate Proof of Ownership (PoW)."""
        # In a real CNS system, this would involve a challenge-response
        # Here we verify the user has the file locally (simulated by having the hash)
        log_action("PoW Verified", f"User {user_id} verified ownership for file hash: {file_hash}")
        return True
